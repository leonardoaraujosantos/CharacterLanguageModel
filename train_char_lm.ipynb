{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Character Language Model\n",
    "Lanugage Models are used for the following tasks:\n",
    "* Check the probability of a sentence to be part of some specific language\n",
    "* Generate Words/Characters given a sentence\n",
    "On this notebook we will train a character langauge model, used to improve the resutls of the handwriting system.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "Language models are evaluated with perplexity metric, that can be calculated by doing the exp of the cross entropy loss\n",
    "* https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/loss/loss.py\n",
    "* https://www.quora.com/In-NLP-why-do-we-use-perplexity-instead-of-the-loss\n",
    "* https://en.wikipedia.org/wiki/Perplexity\n",
    "\n",
    "#### Datasets\n",
    "* https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "\n",
    "#### References:\n",
    "* https://en.wikipedia.org/wiki/Language_model\n",
    "* https://github.com/furkanu/deeplearning.ai-pytorch\n",
    "* https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "* https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "* https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd#file-pytorch_lstm_variable_mini_batches-py\n",
    "* https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 1\n",
      "Num classes: 69\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.distributions\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from tqdm import tqdm\n",
    "import utils_char\n",
    "import utils_char_lm\n",
    "import models\n",
    "import metrics\n",
    "from char_LM_dataset import CharacterLanguageModelDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "num_gpu = 1\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "pickle_filename_train = \"data/char_lang_model_data_train.pkl\"\n",
    "pickle_filename_test = \"data/char_lang_model_data_test.pkl\"\n",
    "codemap = utils_char.load_pickle('./codemap_LM.pickle')\n",
    "num_classes = len(codemap)\n",
    "print('Num classes:', num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "# Pure sequence to sequence models can't deal with batches\n",
    "batch_size = 200\n",
    "clip = 50.0\n",
    "lr = 0.001\n",
    "hidden_size = 256 #256\n",
    "num_layers = 2 #3\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The Song.\n"
     ]
    }
   ],
   "source": [
    "char_dataset_train = CharacterLanguageModelDataset(pickle_filename_train)\n",
    "dataloader_train = utils.DataLoader(char_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "char_dataset_test = CharacterLanguageModelDataset(pickle_filename_test)\n",
    "dataloader_test = utils.DataLoader(char_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "sample = char_dataset_train[0]\n",
    "sample_str = ''.join([utils_char.char_from_class_id(char, codemap) for char in sample['X'][0:sample['len_x']]])\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter('./logs')\n",
    "# Default directory \"runs\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Masked Loss\n",
    "We need to filter out from the gradient the padding values.\n",
    "##### Parameters\n",
    "* input(Decoder output): [batch x sequence]\n",
    "* target(Label): [batch]\n",
    "* mask shape: [batch]\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "input: torch.Size([32, 83])\n",
    "target: torch.Size([32])\n",
    "mask: torch.Size([32])\n",
    "\n",
    "input: torch.Size([70, 84, 69])\n",
    "target: torch.Size([70, 84])\n",
    "mask: torch.Size([70, 84])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(predicted, target, mask):    \n",
    "    # Get total number of valid elements\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    # flatten all the labels, mask and prediction\n",
    "    target = target.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "    \n",
    "    predicted = predicted.view(-1, num_classes)        \n",
    "    \n",
    "    # pick the values for the label and zero out the rest with the mask\n",
    "    #predicted_mask = predicted[range(predicted.shape[0]), target] * mask\n",
    "    predicted_mask = -torch.log(predicted[range(predicted.shape[0]), target])\n",
    "    loss = predicted_mask.masked_select(mask).mean()        \n",
    "        \n",
    "    loss = loss.to(device)    \n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(seq_model, writer = None):\n",
    "    \n",
    "    seq_model.eval()\n",
    "\n",
    "    # Iterate on the test set\n",
    "    print_every = 10\n",
    "    metrics = []\n",
    "    for iter, sample in enumerate(dataloader_test):\n",
    "        input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "        target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "        target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "        len_input = sample['len_x'].to(device)\n",
    "        len_target = sample['len_y'].to(device)\n",
    "        curr_batch_size = len_input.shape[0]\n",
    "        with torch.no_grad():\n",
    "            hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "            # Run words through seq_model (all batch at once)   \n",
    "            seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "\n",
    "            loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)\n",
    "            \n",
    "            # Add predicted/target text to tensorboard\n",
    "            if writer:\n",
    "                pass\n",
    "\n",
    "            metric = loss.item()\n",
    "            \n",
    "            \n",
    "            metrics.append(metric)\n",
    "\n",
    "    distance_test = np.mean(metrics)\n",
    "    return distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, seq_model,  \n",
    "          model_optimizer, iterations):\n",
    "    # Start seq_model hidden state as zero\n",
    "    curr_batch_size = len_input.shape[0]\n",
    "    \n",
    "    hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "\n",
    "    # Zero the gradient for doing backprop\n",
    "    model_optimizer.zero_grad()  \n",
    "\n",
    "    # Initialize Loss\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "        \n",
    "    # Run words through seq_model (all batch at once)   \n",
    "    seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "    \n",
    "    loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)    \n",
    "    \n",
    "    # Calculate the loss gradient wrt to the model weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq_model.parameters(), clip)    \n",
    "\n",
    "    # Do the gradient descent step\n",
    "    model_optimizer.step()    \n",
    "\n",
    "    # Return normalized loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(seq_model, n_epochs=100, learning_rate=0.01):\n",
    "\n",
    "    # Initialize SGD Optimizer to train the network\n",
    "    seq_model_optimizer = optim.Adam(seq_model.parameters(), lr=learning_rate)    \n",
    "    sc_plt_enc = torch.optim.lr_scheduler.ReduceLROnPlateau(seq_model_optimizer, patience=4, verbose=True)    \n",
    "        \n",
    "    iterations = 0\n",
    "    best_metric = 1e12\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        seq_model.train()        \n",
    "        for iter, sample in enumerate(dataloader_train):\n",
    "            # Select a sample and transpose padded arrays into (max_len_sequence x batch_size)            \n",
    "            \n",
    "            # Each 10 iterations send some input data to tensorboard\n",
    "            if iterations % 10 == 0:\n",
    "                pass\n",
    "                # Select first element from the Batch and send to tensorboard\n",
    "                #stroke = sample['sequence'][0].numpy()\n",
    "                #lbl_str = sample['label_str'][0]         \n",
    "                #fig_input = iam.line_plot(stroke, lbl_str, display=False)\n",
    "                #writer.add_figure('train/stroke_target', fig_input, iterations)\n",
    "        \n",
    "            #input_tensor = sample['X'].type(torch.FloatTensor).to(device)\n",
    "            input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "            target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "            target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "            len_input = sample['len_x'].to(device)\n",
    "            len_target = sample['len_y'].to(device)                        \n",
    "                \n",
    "            # Train on that particular input/output sequence\n",
    "            loss = train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, \n",
    "                         seq_model, seq_model_optimizer, iterations)\n",
    "\n",
    "            # Accumulate Loss for display\n",
    "            running_loss += loss\n",
    "\n",
    "            # Send loss to Tensorboard\n",
    "            #print(loss)\n",
    "            writer.add_scalar('train/loss', loss, iterations)\n",
    "            iterations +=1\n",
    "        \n",
    "        # Epoch ended        \n",
    "        sc_plt_enc.step(running_loss)        \n",
    "        writer.add_scalar('train/running_loss', running_loss, epoch)\n",
    "        \n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in seq_model_optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('train/learning_rate', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Evaluate Model\n",
    "        metric = evaluate_test(seq_model, writer)\n",
    "        writer.add_scalar('test/metric_loss', metric, epoch)\n",
    "        \n",
    "        test_metric = metric \n",
    "        #print('Epoch:', epoch, 'Running loss:', running_loss)\n",
    "        \n",
    "        # Save model on the best evaluation distance \n",
    "        if test_metric < best_metric:\n",
    "                print('Smallest metric at epoch:', epoch, 'metric:', test_metric, 'running_loss:', running_loss)\n",
    "                torch.save({'seq_model': seq_model.state_dict(),}, \n",
    "                           os.path.join('./', '{}_{}.pt'.format('model_lm', 'best')))\n",
    "                best_metric = test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to tensorboard hyper-parameter values\n",
    "writer.add_text('train/params', \n",
    "                'learning_rate:' + str(lr) + \n",
    "                ' hidden_size:' + str(hidden_size) + \n",
    "                ' num_layers:' + str(num_layers), 0)\n",
    "\n",
    "# Instantiate Neural Character Language Model Networks\n",
    "char_LM = models.CharLangModel(num_classes, hidden_size, num_classes, num_layers=num_layers).to(device)\n",
    "\n",
    "# Train\n",
    "train_loop(char_LM, n_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Probability of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Thi1):0.000000\n",
      "P(This):0.012076\n",
      "P(Love):0.000206\n",
      "P(Lov1):0.000000\n",
      "P(Hellz):0.000000\n",
      "P(Hello):0.000000\n",
      "P(HellO):0.000000\n",
      "P(HeLlo):0.000000\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['Thi1', 'This', 'Love', 'Lov1', 'Hellz', 'Hello', 'HellO', 'HeLlo']\n",
    "for word in lst_words:\n",
    "    print('P(%s):%f' % (word, utils_char_lm.getProbabilitySentence(word, char_LM, device, codemap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Characters Given some chars\n",
    "##### References:\n",
    "* https://pytorch.org/docs/stable/distributions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he PRINCESS OF FRANCE, and SIR HUGH EVANS<EOS>\n",
      "ing the Duke of ANTIPHOLUS OFFICERS, DUKE OF SUFFOLK<EOS>\n",
      " the basilisk  As the second Duke of Alexas and the Prince and Cornet.<EOS>\n",
      "e and Sir John and Cassius.<EOS>\n",
      "anting<EOS>\n",
      " the matter that the second court of the court<EOS>\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap)\n",
    "    res_str = ''.join([utils_char.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(t)heirn SIR JOHN MARDUS,<EOS>\n",
      "(k)ipp my lady- shall be not a<EOS>\n",
      "(de)atus, nature's contempt, Hermia, Dick,<EOS>\n",
      "(Liv)in herself, and rough as gentlemen at Clifford. Ha<EOS>\n",
      "(Pre)anshire<EOS>\n",
      "(To be or)dant offer before; behind, news,<EOS>\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap, greedly=False)\n",
    "    res_str = ''.join([utils_char.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
