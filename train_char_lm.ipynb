{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Character Language Model\n",
    "Language Models are used for the following tasks:\n",
    "* Check the probability of a sentence to be part of some specific language\n",
    "* Generate Words/Characters given a sentence\n",
    "On this notebook we will train a Neural Character Language model based on GRUs.\n",
    "![Input Output](docs/imgs/char_lang_model.png \"Title\")\n",
    "\n",
    "#### Evaluation Metrics\n",
    "Character Language models are evaluated with Character Error Rate metric, that can be calculated by doing the exp of the cross entropy loss\n",
    "\n",
    "#### Datasets\n",
    "* https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "\n",
    "#### References:\n",
    "* https://en.wikipedia.org/wiki/Language_model\n",
    "* https://github.com/furkanu/deeplearning.ai-pytorch\n",
    "* https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "* https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 1\n",
      "Num classes: 69\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.distributions\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from tqdm import tqdm\n",
    "import utils_char_dataset\n",
    "import utils_char_lm\n",
    "import model as models\n",
    "from char_LM_dataset import CharacterLanguageModelDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "num_gpu = 1\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "pickle_filename_train = \"data/shakespeare_corpus_data_train.pkl\"\n",
    "pickle_filename_test = \"data/shakespeare_corpus_data_test.pkl\"\n",
    "codemap = utils_char_dataset.load_pickle('./codemap_LM.pickle')\n",
    "num_classes = len(codemap)\n",
    "print('Num classes:', num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "# Pure sequence to sequence models can't deal with batches\n",
    "batch_size = 200\n",
    "clip = 50.0\n",
    "lr = 0.001\n",
    "hidden_size = 256 #256\n",
    "num_layers = 2 #3\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "char_dataset_train = CharacterLanguageModelDataset(pickle_filename_train)\n",
    "dataloader_train = utils.DataLoader(char_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "char_dataset_test = CharacterLanguageModelDataset(pickle_filename_test)\n",
    "dataloader_test = utils.DataLoader(char_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "sample = char_dataset_train[0]\n",
    "sample_str = ''.join([utils_char_dataset.char_from_class_id(char, codemap) for char in sample['X'][0:sample['len_x']]])\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter('./logs')\n",
    "# Default directory \"runs\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Masked Loss\n",
    "We need to filter out from the gradient the padding values.\n",
    "##### Parameters\n",
    "* input(Decoder output): [batch x sequence]\n",
    "* target(Label): [batch]\n",
    "* mask shape: [batch]\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "input: torch.Size([32, 83])\n",
    "target: torch.Size([32])\n",
    "mask: torch.Size([32])\n",
    "\n",
    "input: torch.Size([70, 84, 69])\n",
    "target: torch.Size([70, 84])\n",
    "mask: torch.Size([70, 84])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(predicted, target, mask):    \n",
    "    # Get total number of valid elements\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    # flatten all the labels, mask and prediction\n",
    "    target = target.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "    \n",
    "    predicted = predicted.view(-1, num_classes)        \n",
    "    \n",
    "    # pick the values for the label and zero out the rest with the mask\n",
    "    #predicted_mask = predicted[range(predicted.shape[0]), target] * mask\n",
    "    predicted_mask = -torch.log(predicted[range(predicted.shape[0]), target])\n",
    "    loss = predicted_mask.masked_select(mask).mean()        \n",
    "        \n",
    "    loss = loss.to(device)    \n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(seq_model, writer = None):\n",
    "    \n",
    "    seq_model.eval()\n",
    "\n",
    "    # Iterate on the test set\n",
    "    print_every = 10\n",
    "    metrics = []\n",
    "    for iter, sample in enumerate(dataloader_test):\n",
    "        input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "        target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "        target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "        len_input = sample['len_x'].to(device)\n",
    "        len_target = sample['len_y'].to(device)\n",
    "        curr_batch_size = len_input.shape[0]\n",
    "        with torch.no_grad():\n",
    "            hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "            # Run words through seq_model (all batch at once)   \n",
    "            seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "\n",
    "            loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)\n",
    "            \n",
    "            # Add predicted/target text to tensorboard\n",
    "            if writer:\n",
    "                pass\n",
    "\n",
    "            metric = loss.item()\n",
    "            \n",
    "            \n",
    "            metrics.append(metric)\n",
    "\n",
    "    distance_test = np.mean(metrics)\n",
    "    return distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, seq_model,  \n",
    "          model_optimizer, iterations):\n",
    "    # Start seq_model hidden state as zero\n",
    "    curr_batch_size = len_input.shape[0]\n",
    "    \n",
    "    hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "\n",
    "    # Zero the gradient for doing backprop\n",
    "    model_optimizer.zero_grad()  \n",
    "\n",
    "    # Initialize Loss\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "        \n",
    "    # Run words through seq_model (all batch at once)   \n",
    "    seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "    \n",
    "    loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)    \n",
    "    \n",
    "    # Calculate the loss gradient wrt to the model weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq_model.parameters(), clip)    \n",
    "\n",
    "    # Do the gradient descent step\n",
    "    model_optimizer.step()    \n",
    "\n",
    "    # Return normalized loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(seq_model, n_epochs=100, learning_rate=0.01):\n",
    "\n",
    "    # Initialize SGD Optimizer to train the network\n",
    "    seq_model_optimizer = optim.Adam(seq_model.parameters(), lr=learning_rate)    \n",
    "    sc_plt_enc = torch.optim.lr_scheduler.ReduceLROnPlateau(seq_model_optimizer, patience=4, verbose=True)    \n",
    "        \n",
    "    iterations = 0\n",
    "    best_metric = 1e12\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        seq_model.train()        \n",
    "        for iter, sample in enumerate(dataloader_train):\n",
    "            # Select a sample and transpose padded arrays into (max_len_sequence x batch_size)            \n",
    "            \n",
    "            # Each 10 iterations send some input data to tensorboard\n",
    "            if iterations % 10 == 0:\n",
    "                pass\n",
    "                # Select first element from the Batch and send to tensorboard\n",
    "                #stroke = sample['sequence'][0].numpy()\n",
    "                #lbl_str = sample['label_str'][0]         \n",
    "                #fig_input = iam.line_plot(stroke, lbl_str, display=False)\n",
    "                #writer.add_figure('train/stroke_target', fig_input, iterations)\n",
    "        \n",
    "            #input_tensor = sample['X'].type(torch.FloatTensor).to(device)\n",
    "            input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "            target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "            target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "            len_input = sample['len_x'].to(device)\n",
    "            len_target = sample['len_y'].to(device)                        \n",
    "                \n",
    "            # Train on that particular input/output sequence\n",
    "            loss = train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, \n",
    "                         seq_model, seq_model_optimizer, iterations)\n",
    "\n",
    "            # Accumulate Loss for display\n",
    "            running_loss += loss\n",
    "\n",
    "            # Send loss to Tensorboard\n",
    "            #print(loss)\n",
    "            writer.add_scalar('train/loss', loss, iterations)\n",
    "            iterations +=1\n",
    "        \n",
    "        # Epoch ended        \n",
    "        sc_plt_enc.step(running_loss)        \n",
    "        writer.add_scalar('train/running_loss', running_loss, epoch)\n",
    "        \n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in seq_model_optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('train/learning_rate', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Evaluate Model\n",
    "        metric = evaluate_test(seq_model, writer)\n",
    "        writer.add_scalar('test/metric_loss', metric, epoch)\n",
    "        \n",
    "        test_metric = metric \n",
    "        #print('Epoch:', epoch, 'Running loss:', running_loss)\n",
    "        \n",
    "        # Save model on the best evaluation distance \n",
    "        if test_metric < best_metric:\n",
    "                print('Smallest metric at epoch:', epoch, 'metric:', test_metric, 'running_loss:', running_loss)\n",
    "                torch.save({'seq_model': seq_model.state_dict(),}, \n",
    "                           os.path.join('./', '{}_{}.pt'.format('model_lm', 'best')))\n",
    "                best_metric = test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest metric at epoch: 0 metric: 1.126661451190644 running_loss: 5218.216045677662\n",
      "Smallest metric at epoch: 1 metric: 1.0934671594404064 running_loss: 4478.075229525566\n",
      "Smallest metric at epoch: 2 metric: 1.0806190542001153 running_loss: 4387.168428063393\n",
      "Smallest metric at epoch: 3 metric: 1.0749600771525483 running_loss: 4344.22549033165\n",
      "Smallest metric at epoch: 4 metric: 1.0704284019586516 running_loss: 4318.8025180101395\n",
      "Smallest metric at epoch: 5 metric: 1.0683795985520017 running_loss: 4302.5816569924355\n",
      "Smallest metric at epoch: 6 metric: 1.0659847008415442 running_loss: 4291.340185821056\n",
      "Smallest metric at epoch: 7 metric: 1.0643659338454186 running_loss: 4283.761633038521\n"
     ]
    }
   ],
   "source": [
    "# Add to tensorboard hyper-parameter values\n",
    "writer.add_text('train/params', \n",
    "                'learning_rate:' + str(lr) + \n",
    "                ' hidden_size:' + str(hidden_size) + \n",
    "                ' num_layers:' + str(num_layers), 0)\n",
    "\n",
    "# Instantiate Neural Character Language Model Networks\n",
    "char_LM = models.CharLangModel(num_classes, hidden_size, num_classes, num_layers=num_layers).to(device)\n",
    "\n",
    "# Train\n",
    "train_loop(char_LM, n_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Probability of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = ['Thi1', 'This', 'Love', 'Lov1', 'Hellz', 'Hello', 'HellO', 'HeLlo']\n",
    "for word in lst_words:\n",
    "    print('P(%s):%f' % (word, utils_char_lm.getProbabilitySentence(word, char_LM, device, codemap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Characters Given some chars\n",
    "##### References:\n",
    "* https://pytorch.org/docs/stable/distributions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap)\n",
    "    res_str = ''.join([utils_char.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap, greedly=False)\n",
    "    res_str = ''.join([utils_char.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
