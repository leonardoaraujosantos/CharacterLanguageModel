{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Character Language Model\n",
    "Language Models are used for the following tasks:\n",
    "* Check the probability of a sentence to be part of some specific language\n",
    "* Generate Words/Characters given a sentence\n",
    "On this notebook we will train a Neural Character Language model based on GRUs.\n",
    "![Input Output](docs/imgs/char_lang_model.png \"Title\")\n",
    "\n",
    "#### Character Generation\n",
    "One way to generate data is to input a character, then sample the output of the RNN and feed this result back to the RNN\n",
    "![Generation](docs/imgs/char_lang_model_gen.png \"Title\")\n",
    "\n",
    "#### Evaluation Metrics\n",
    "Character Language models are evaluated with Character Error Rate metric, that can be calculated by doing the exp of the cross entropy loss\n",
    "\n",
    "#### Training\n",
    "During training we use \"masked cross entropy\" to ignore pad values and the \"plateau\" learning rate scheduler that drop the learning rate if the loss functions doesnt drop for few epochs (patience).\n",
    "\n",
    "![Loss](docs/imgs/loss_plateau_scheduler.png \"Title\")\n",
    "\n",
    "#### Datasets\n",
    "* https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "\n",
    "#### References:\n",
    "* https://en.wikipedia.org/wiki/Language_model\n",
    "* https://github.com/furkanu/deeplearning.ai-pytorch\n",
    "* https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "* https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 1\n",
      "class_eos_id: 69\n",
      "Num classes: 69\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import utils_char_dataset\n",
    "import utils_char_lm\n",
    "import models\n",
    "from char_LM_dataset import CharacterLanguageModelDataset\n",
    "import metrics\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "num_gpu = 1\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "pickle_filename_train = \"data/shakespeare_corpus_data_train.pkl\"\n",
    "pickle_filename_test = \"data/shakespeare_corpus_data_test.pkl\"\n",
    "\n",
    "# The codemap is a dictionary of words to class index\n",
    "codemap = utils_char_dataset.load_pickle('./codemap_LM.pickle')\n",
    "class_eos = utils_char_dataset.EOS_token\n",
    "print('class_eos_id:', class_eos)\n",
    "num_classes = len(codemap)\n",
    "print('Num classes:', num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "# Pure sequence to sequence models can't deal with batches\n",
    "batch_size = 200\n",
    "clip = 50.0\n",
    "lr = 0.001\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "char_dataset_train = CharacterLanguageModelDataset(pickle_filename_train)\n",
    "dataloader_train = utils.DataLoader(char_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "char_dataset_test = CharacterLanguageModelDataset(pickle_filename_test)\n",
    "dataloader_test = utils.DataLoader(char_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "sample = char_dataset_train[0]\n",
    "sample_str = ''.join([utils_char_dataset.char_from_class_id(char, codemap) for char in sample['X'][0:sample['len_x']]])\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter('./logs')\n",
    "# Default directory \"runs\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Masked Loss\n",
    "We need to filter out from the gradient the padding values.\n",
    "##### Parameters\n",
    "* input(Decoder output): [batch x sequence]\n",
    "* target(Label): [batch]\n",
    "* mask shape: [batch]\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "input: torch.Size([32, 83])\n",
    "target: torch.Size([32])\n",
    "mask: torch.Size([32])\n",
    "\n",
    "input: torch.Size([70, 84, 69])\n",
    "target: torch.Size([70, 84])\n",
    "mask: torch.Size([70, 84])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(predicted, target, mask):    \n",
    "    # Get total number of valid elements\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    # flatten all the labels, mask and prediction\n",
    "    target = target.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "    \n",
    "    predicted = predicted.view(-1, num_classes)        \n",
    "    \n",
    "    # pick the values for the label and zero out the rest with the mask\n",
    "    #predicted_mask = predicted[range(predicted.shape[0]), target] * mask\n",
    "    predicted_mask = -torch.log(predicted[range(predicted.shape[0]), target])\n",
    "    loss = predicted_mask.masked_select(mask).mean()        \n",
    "        \n",
    "    loss = loss.to(device)    \n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(seq_model, writer = None):\n",
    "    \n",
    "    seq_model.eval()\n",
    "\n",
    "    # Iterate on the test set\n",
    "    metrics_list = []\n",
    "    for iter, sample in enumerate(dataloader_test):\n",
    "        input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "        target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "        target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "        len_input = sample['len_x'].to(device)\n",
    "        len_target = sample['len_y'].to(device)\n",
    "        curr_batch_size = len_input.shape[0]\n",
    "        with torch.no_grad():\n",
    "            hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "            # Run words through seq_model (all batch at once)   \n",
    "            seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)            \n",
    "\n",
    "            loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)\n",
    "            \n",
    "            # Add predicted/target text to tensorboard\n",
    "            if writer:\n",
    "                pass\n",
    "            # Get next character from Model\n",
    "            scores, greedy_class = torch.max(seq_model_outputs, dim=2)             \n",
    "            \n",
    "            # Transform character class ID to character strings\n",
    "            characters_idx = greedy_class.cpu().numpy().tolist()\n",
    "            characters_idx_label = target_tensor.cpu().numpy().tolist()            \n",
    "            characters = [[utils_char_dataset.char_from_class_id(char, codemap) for char in batch] for batch in characters_idx]\n",
    "            characters_label = [[utils_char_dataset.char_from_class_id(char, codemap) for char in batch] for batch in characters_idx_label]\n",
    "            \n",
    "            # Crop until EOS\n",
    "            characters_crop = []\n",
    "            characters_label_crop = []\n",
    "            for batch in characters:\n",
    "                for idx in range(len(batch)):\n",
    "                    char = batch[idx]                    \n",
    "                    if char != '<EOS>':\n",
    "                        characters_crop.append(char)\n",
    "                    else:\n",
    "                        break\n",
    "            for batch in characters_label:\n",
    "                for idx in range(len(characters_label)):\n",
    "                    char = batch[idx]                    \n",
    "                    if char != '<EOS>':\n",
    "                        characters_label_crop.append(char)\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # Calculate Character Error Rate on batch \n",
    "            cer_batch = [metrics.cer(pred,targ) for pred,targ in zip(characters_crop, characters_label_crop)]\n",
    "            metric = np.mean(cer_batch)\n",
    "            metrics_list.append(metric)\n",
    "\n",
    "    distance_test = np.mean(metrics_list)\n",
    "    return distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, seq_model,  \n",
    "          model_optimizer, iterations):\n",
    "    # Start seq_model hidden state as zero\n",
    "    curr_batch_size = len_input.shape[0]\n",
    "    \n",
    "    hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "\n",
    "    # Zero the gradient for doing backprop\n",
    "    model_optimizer.zero_grad()  \n",
    "\n",
    "    # Initialize Loss\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "        \n",
    "    # Run words through seq_model (all batch at once)   \n",
    "    seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "    \n",
    "    loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)    \n",
    "    \n",
    "    # Calculate the loss gradient wrt to the model weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq_model.parameters(), clip)    \n",
    "\n",
    "    # Do the gradient descent step\n",
    "    model_optimizer.step()    \n",
    "\n",
    "    # Return normalized loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(seq_model, n_epochs=100, learning_rate=0.01):\n",
    "\n",
    "    # Initialize SGD Optimizer to train the network\n",
    "    seq_model_optimizer = optim.Adam(seq_model.parameters(), lr=learning_rate)    \n",
    "    sc_plt_enc = torch.optim.lr_scheduler.ReduceLROnPlateau(seq_model_optimizer, patience=2, verbose=True)    \n",
    "        \n",
    "    iterations = 0\n",
    "    best_metric = 1e12\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        seq_model.train()        \n",
    "        for iter, sample in enumerate(dataloader_train):\n",
    "            # Select a sample          \n",
    "            input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "            target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "            target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "            len_input = sample['len_x'].to(device)\n",
    "            len_target = sample['len_y'].to(device)                        \n",
    "                \n",
    "            # Train on that particular input/output sequence\n",
    "            loss = train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, \n",
    "                         seq_model, seq_model_optimizer, iterations)\n",
    "\n",
    "            # Accumulate Loss for display\n",
    "            running_loss += loss\n",
    "\n",
    "            # Send loss to Tensorboard\n",
    "            writer.add_scalar('train/loss', loss, iterations)\n",
    "            iterations +=1\n",
    "        \n",
    "        # Epoch ended        \n",
    "        sc_plt_enc.step(running_loss)        \n",
    "        writer.add_scalar('train/running_loss', running_loss, epoch)\n",
    "        \n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in seq_model_optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('train/learning_rate', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Evaluate Model\n",
    "        metric = evaluate_test(seq_model, writer)\n",
    "        writer.add_scalar('test/CER', metric, epoch)\n",
    "        \n",
    "        test_metric = metric \n",
    "        #print('Epoch:', epoch, 'Running loss:', running_loss)\n",
    "        \n",
    "        # Save model on the best evaluation distance \n",
    "        if test_metric < best_metric:\n",
    "                print('Smallest metric at epoch:', epoch, 'metric:', test_metric, 'running_loss:', running_loss)\n",
    "                torch.save({'seq_model': seq_model.state_dict(),}, \n",
    "                           os.path.join('./', '{}_{}.pt'.format('model_lm', 'best')))\n",
    "                best_metric = test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest metric at epoch: 0 metric: 0.9328786754980419 running_loss: 5424.557478547096\n",
      "Smallest metric at epoch: 1 metric: 0.9307224881072942 running_loss: 4662.7906794548035\n",
      "Smallest metric at epoch: 4 metric: 0.9301809330350593 running_loss: 4493.1122079491615\n",
      "Smallest metric at epoch: 7 metric: 0.9294871600464628 running_loss: 4463.07191234827\n"
     ]
    }
   ],
   "source": [
    "# Add to tensorboard hyper-parameter values\n",
    "writer.add_text('train/params', \n",
    "                'learning_rate:' + str(lr) + \n",
    "                ' hidden_size:' + str(hidden_size) + \n",
    "                ' num_layers:' + str(num_layers), 0)\n",
    "\n",
    "# Instantiate Neural Character Language Model Networks\n",
    "char_LM = models.CharLangModel(num_classes, hidden_size, num_classes, num_layers=num_layers).to(device)\n",
    "\n",
    "# Train\n",
    "train_loop(char_LM, n_epochs=epochs, learning_rate=lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
