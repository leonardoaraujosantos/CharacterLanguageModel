{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Character Language Model\n",
    "Language Models are used for the following tasks:\n",
    "* Check the probability of a sentence to be part of some specific language\n",
    "* Generate Words/Characters given a sentence\n",
    "On this notebook we will train a Neural Character Language model based on GRUs.\n",
    "![Input Output](docs/imgs/char_lang_model.png \"Title\")\n",
    "\n",
    "#### Character Generation\n",
    "One way to generate data is to input a character, then sample the output of the RNN and feed this result back to the RNN\n",
    "![Generation](docs/imgs/char_lang_model_gen.png \"Title\")\n",
    "\n",
    "#### Evaluation Metrics\n",
    "Character Language models are evaluated with Character Error Rate metric, that can be calculated by doing the exp of the cross entropy loss\n",
    "\n",
    "#### Training\n",
    "During training we use \"masked cross entropy\" to ignore pad values and the \"plateau\" learning rate scheduler that drop the learning rate if the loss functions doesnt drop for few epochs (patience).\n",
    "\n",
    "![Loss](docs/imgs/loss_plateau_scheduler.png \"Title\")\n",
    "\n",
    "#### Datasets\n",
    "* https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "\n",
    "#### References:\n",
    "* https://en.wikipedia.org/wiki/Language_model\n",
    "* https://github.com/furkanu/deeplearning.ai-pytorch\n",
    "* https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "* https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 1\n",
      "Num classes: 69\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as utils\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import utils_char_dataset\n",
    "import utils_char_lm\n",
    "import models\n",
    "from char_LM_dataset import CharacterLanguageModelDataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "num_gpu = 1\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "pickle_filename_train = \"data/shakespeare_corpus_data_train.pkl\"\n",
    "pickle_filename_test = \"data/shakespeare_corpus_data_test.pkl\"\n",
    "\n",
    "# The codemap is a dictionary of words to class index\n",
    "codemap = utils_char_dataset.load_pickle('./codemap_LM.pickle')\n",
    "num_classes = len(codemap)\n",
    "print('Num classes:', num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "# Pure sequence to sequence models can't deal with batches\n",
    "batch_size = 200\n",
    "clip = 50.0\n",
    "lr = 0.001\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "char_dataset_train = CharacterLanguageModelDataset(pickle_filename_train)\n",
    "dataloader_train = utils.DataLoader(char_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "char_dataset_test = CharacterLanguageModelDataset(pickle_filename_test)\n",
    "dataloader_test = utils.DataLoader(char_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "sample = char_dataset_train[0]\n",
    "sample_str = ''.join([utils_char_dataset.char_from_class_id(char, codemap) for char in sample['X'][0:sample['len_x']]])\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter('./logs')\n",
    "# Default directory \"runs\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Masked Loss\n",
    "We need to filter out from the gradient the padding values.\n",
    "##### Parameters\n",
    "* input(Decoder output): [batch x sequence]\n",
    "* target(Label): [batch]\n",
    "* mask shape: [batch]\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "input: torch.Size([32, 83])\n",
    "target: torch.Size([32])\n",
    "mask: torch.Size([32])\n",
    "\n",
    "input: torch.Size([70, 84, 69])\n",
    "target: torch.Size([70, 84])\n",
    "mask: torch.Size([70, 84])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(predicted, target, mask):    \n",
    "    # Get total number of valid elements\n",
    "    nTotal = mask.sum()\n",
    "    \n",
    "    # flatten all the labels, mask and prediction\n",
    "    target = target.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "    \n",
    "    predicted = predicted.view(-1, num_classes)        \n",
    "    \n",
    "    # pick the values for the label and zero out the rest with the mask\n",
    "    #predicted_mask = predicted[range(predicted.shape[0]), target] * mask\n",
    "    predicted_mask = -torch.log(predicted[range(predicted.shape[0]), target])\n",
    "    loss = predicted_mask.masked_select(mask).mean()        \n",
    "        \n",
    "    loss = loss.to(device)    \n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(seq_model, writer = None):\n",
    "    \n",
    "    seq_model.eval()\n",
    "\n",
    "    # Iterate on the test set\n",
    "    print_every = 10\n",
    "    metrics = []\n",
    "    for iter, sample in enumerate(dataloader_test):\n",
    "        input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "        target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "        target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "        len_input = sample['len_x'].to(device)\n",
    "        len_target = sample['len_y'].to(device)\n",
    "        curr_batch_size = len_input.shape[0]\n",
    "        with torch.no_grad():\n",
    "            hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "            # Run words through seq_model (all batch at once)   \n",
    "            seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "\n",
    "            loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)\n",
    "            \n",
    "            # Add predicted/target text to tensorboard\n",
    "            if writer:\n",
    "                pass\n",
    "\n",
    "            metric = loss.item()\n",
    "            \n",
    "            \n",
    "            metrics.append(metric)\n",
    "\n",
    "    distance_test = np.mean(metrics)\n",
    "    return distance_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, seq_model,  \n",
    "          model_optimizer, iterations):\n",
    "    # Start seq_model hidden state as zero\n",
    "    curr_batch_size = len_input.shape[0]\n",
    "    \n",
    "    hidden_state = models.initHidden(curr_batch_size, False, hidden_size, num_layers, device)\n",
    "\n",
    "    # Zero the gradient for doing backprop\n",
    "    model_optimizer.zero_grad()  \n",
    "\n",
    "    # Initialize Loss\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "        \n",
    "    # Run words through seq_model (all batch at once)   \n",
    "    seq_model_outputs, hidden_state = seq_model(input_tensor, hidden_state, len_input)        \n",
    "    \n",
    "    loss, nTotal = maskNLLLoss(seq_model_outputs, target_tensor, target_mask)    \n",
    "    \n",
    "    # Calculate the loss gradient wrt to the model weights\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(seq_model.parameters(), clip)    \n",
    "\n",
    "    # Do the gradient descent step\n",
    "    model_optimizer.step()    \n",
    "\n",
    "    # Return normalized loss\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(seq_model, n_epochs=100, learning_rate=0.01):\n",
    "\n",
    "    # Initialize SGD Optimizer to train the network\n",
    "    seq_model_optimizer = optim.Adam(seq_model.parameters(), lr=learning_rate)    \n",
    "    sc_plt_enc = torch.optim.lr_scheduler.ReduceLROnPlateau(seq_model_optimizer, patience=2, verbose=True)    \n",
    "        \n",
    "    iterations = 0\n",
    "    best_metric = 1e12\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        seq_model.train()        \n",
    "        for iter, sample in enumerate(dataloader_train):\n",
    "            # Select a sample and transpose padded arrays into (max_len_sequence x batch_size)            \n",
    "            \n",
    "            # Each 10 iterations send some input data to tensorboard\n",
    "            if iterations % 10 == 0:\n",
    "                pass\n",
    "                # Select first element from the Batch and send to tensorboard\n",
    "                #stroke = sample['sequence'][0].numpy()\n",
    "                #lbl_str = sample['label_str'][0]         \n",
    "                #fig_input = iam.line_plot(stroke, lbl_str, display=False)\n",
    "                #writer.add_figure('train/stroke_target', fig_input, iterations)\n",
    "        \n",
    "            #input_tensor = sample['X'].type(torch.FloatTensor).to(device)\n",
    "            input_tensor = sample['X'].type(torch.LongTensor).to(device)\n",
    "            target_tensor = sample['Y'].type(torch.LongTensor).to(device)\n",
    "            target_mask = sample['label_mask'].type(torch.ByteTensor).to(device)\n",
    "            len_input = sample['len_x'].to(device)\n",
    "            len_target = sample['len_y'].to(device)                        \n",
    "                \n",
    "            # Train on that particular input/output sequence\n",
    "            loss = train_batch(input_tensor, target_tensor, len_input, len_target, target_mask, \n",
    "                         seq_model, seq_model_optimizer, iterations)\n",
    "\n",
    "            # Accumulate Loss for display\n",
    "            running_loss += loss\n",
    "\n",
    "            # Send loss to Tensorboard\n",
    "            #print(loss)\n",
    "            writer.add_scalar('train/loss', loss, iterations)\n",
    "            iterations +=1\n",
    "        \n",
    "        # Epoch ended        \n",
    "        sc_plt_enc.step(running_loss)        \n",
    "        writer.add_scalar('train/running_loss', running_loss, epoch)\n",
    "        \n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in seq_model_optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('train/learning_rate', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Evaluate Model\n",
    "        metric = evaluate_test(seq_model, writer)\n",
    "        writer.add_scalar('test/metric_loss', metric, epoch)\n",
    "        \n",
    "        test_metric = metric \n",
    "        #print('Epoch:', epoch, 'Running loss:', running_loss)\n",
    "        \n",
    "        # Save model on the best evaluation distance \n",
    "        if test_metric < best_metric:\n",
    "                print('Smallest metric at epoch:', epoch, 'metric:', test_metric, 'running_loss:', running_loss)\n",
    "                torch.save({'seq_model': seq_model.state_dict(),}, \n",
    "                           os.path.join('./', '{}_{}.pt'.format('model_lm', 'best')))\n",
    "                best_metric = test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest metric at epoch: 0 metric: 1.1425018786326744 running_loss: 5438.66271853447\n",
      "Smallest metric at epoch: 1 metric: 1.1075885187495838 running_loss: 4665.277022123337\n",
      "Smallest metric at epoch: 2 metric: 1.095850261642769 running_loss: 4564.642035484314\n",
      "Smallest metric at epoch: 3 metric: 1.0876045322206755 running_loss: 4519.386776983738\n",
      "Smallest metric at epoch: 4 metric: 1.0823433448100037 running_loss: 4493.608509123325\n",
      "Smallest metric at epoch: 6 metric: 1.077851315956158 running_loss: 4467.401553630829\n",
      "Smallest metric at epoch: 8 metric: 1.0776235544496524 running_loss: 4459.083432257175\n",
      "Smallest metric at epoch: 10 metric: 1.0774076803031887 running_loss: 4456.999837517738\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Smallest metric at epoch: 13 metric: 1.0578111486794415 running_loss: 4368.681524336338\n",
      "Smallest metric at epoch: 14 metric: 1.0542610906180152 running_loss: 4329.041776061058\n",
      "Smallest metric at epoch: 15 metric: 1.0520535281915093 running_loss: 4311.605188965797\n",
      "Smallest metric at epoch: 16 metric: 1.0506718967547701 running_loss: 4300.176759183407\n",
      "Smallest metric at epoch: 17 metric: 1.049131089594306 running_loss: 4291.82405847311\n",
      "Smallest metric at epoch: 18 metric: 1.048130599067375 running_loss: 4284.181529581547\n",
      "Smallest metric at epoch: 19 metric: 1.0470877154967737 running_loss: 4278.622231662273\n",
      "Smallest metric at epoch: 20 metric: 1.0467063968303199 running_loss: 4273.649494707584\n",
      "Smallest metric at epoch: 21 metric: 1.0457363213245197 running_loss: 4269.021693587303\n",
      "Smallest metric at epoch: 22 metric: 1.0453019151402154 running_loss: 4265.3727578520775\n",
      "Smallest metric at epoch: 23 metric: 1.0449929421069617 running_loss: 4261.097580730915\n",
      "Smallest metric at epoch: 24 metric: 1.04460670189424 running_loss: 4258.576782882214\n",
      "Smallest metric at epoch: 25 metric: 1.0437404579439606 running_loss: 4254.908811748028\n",
      "Smallest metric at epoch: 26 metric: 1.0432205772452767 running_loss: 4253.292358338833\n",
      "Smallest metric at epoch: 27 metric: 1.0426819099819582 running_loss: 4249.237896561623\n",
      "Smallest metric at epoch: 28 metric: 1.0422578024494147 running_loss: 4246.014488458633\n",
      "Smallest metric at epoch: 29 metric: 1.0422330965223967 running_loss: 4243.4661319851875\n",
      "Smallest metric at epoch: 30 metric: 1.0418024240206192 running_loss: 4242.357078790665\n",
      "Smallest metric at epoch: 31 metric: 1.0413235755030172 running_loss: 4239.860238671303\n",
      "Smallest metric at epoch: 32 metric: 1.0412614427490403 running_loss: 4237.0729095339775\n",
      "Smallest metric at epoch: 33 metric: 1.0408559035831968 running_loss: 4235.470750033855\n",
      "Smallest metric at epoch: 34 metric: 1.0406935952455136 running_loss: 4234.274294912815\n",
      "Smallest metric at epoch: 35 metric: 1.040528173060745 running_loss: 4232.330143749714\n",
      "Smallest metric at epoch: 36 metric: 1.0402517937239417 running_loss: 4230.204791605473\n",
      "Smallest metric at epoch: 37 metric: 1.0398744618284728 running_loss: 4228.548419177532\n",
      "Smallest metric at epoch: 39 metric: 1.03977137143226 running_loss: 4225.333808422089\n",
      "Smallest metric at epoch: 40 metric: 1.0391943602762836 running_loss: 4223.5772818923\n",
      "Smallest metric at epoch: 41 metric: 1.0390551236939272 running_loss: 4221.970109462738\n",
      "Smallest metric at epoch: 42 metric: 1.0388150608988929 running_loss: 4221.285155892372\n",
      "Smallest metric at epoch: 43 metric: 1.0388004795675003 running_loss: 4219.806967437267\n",
      "Smallest metric at epoch: 44 metric: 1.0386880818597493 running_loss: 4218.279934346676\n",
      "Smallest metric at epoch: 45 metric: 1.0383787642825733 running_loss: 4216.8161481022835\n",
      "Smallest metric at epoch: 47 metric: 1.0383672526564671 running_loss: 4213.868463516235\n",
      "Smallest metric at epoch: 48 metric: 1.0378830694570773 running_loss: 4213.599335432053\n"
     ]
    }
   ],
   "source": [
    "# Add to tensorboard hyper-parameter values\n",
    "writer.add_text('train/params', \n",
    "                'learning_rate:' + str(lr) + \n",
    "                ' hidden_size:' + str(hidden_size) + \n",
    "                ' num_layers:' + str(num_layers), 0)\n",
    "\n",
    "# Instantiate Neural Character Language Model Networks\n",
    "char_LM = models.CharLangModel(num_classes, hidden_size, num_classes, num_layers=num_layers).to(device)\n",
    "\n",
    "# Train\n",
    "train_loop(char_LM, n_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Probability of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Thi1):0.000000\n",
      "P(This):0.051411\n",
      "P(Love):0.021022\n",
      "P(Lov1):0.000000\n",
      "P(Hellz):0.000000\n",
      "P(Hello):0.000000\n",
      "P(HellO):0.000000\n",
      "P(HeLlo):0.000000\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['Thi1', 'This', 'Love', 'Lov1', 'Hellz', 'Hello', 'HellO', 'HeLlo']\n",
    "for word in lst_words:\n",
    "    print('P(%s):%f' % (word, utils_char_lm.getProbabilitySentence(word, char_LM, device, codemap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Characters Given some chars\n",
    "##### References:\n",
    "* https://pytorch.org/docs/stable/distributions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(t)he<EOS>\n",
      "(k)now<EOS>\n",
      "(de)ath<EOS>\n",
      "(Liv)e<EOS>\n",
      "(Pre)sent<EOS>\n",
      "(To be or)s<EOS>\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap)\n",
    "    res_str = ''.join([utils_char_dataset.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(t)hen<EOS>\n",
      "(k)now<EOS>\n",
      "(de)ath<EOS>\n",
      "(Liv)es<EOS>\n",
      "(Pre)sence<EOS>\n",
      "(To be or not to b)'s<EOS>\n"
     ]
    }
   ],
   "source": [
    "lst_words = ['t', 'k', 'de', 'Liv', 'Pre', 'To be or not to b']\n",
    "for word in lst_words:\n",
    "    pred = utils_char_lm.getNextChar(word,100, char_LM, device, codemap, greedly=False)\n",
    "    res_str = ''.join([utils_char_dataset.char_from_class_id(class_id, codemap) for class_id in pred])\n",
    "    print('('+word+')'+res_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
